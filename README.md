# cf_ai_chat_memory

ğŸš€ **[Live Demo](https://c7969915.cf-ai-chat-memory.pages.dev/)**

**A persistent AI chat assistant built entirely on Cloudflare** â€” the platform for edge computing, serverless inference, and distributed state management.

- **LLM:** Llama 3.1 8B via Workers AI (recommended by Cloudflare assignment)
- **Workflow/Coordination:** Cloudflare Workers (edge compute)
- **User Input:** Modern web chat UI with real-time responses
- **Memory/State:** Cloudflare KV (per-session persistent history)
- **Architecture:** Serverless, globally distributed, pay-as-you-go
- **Repo Requirement:** âœ… Name starts with `cf_ai_`

---

## ğŸ¯ What It Does

An AI chat assistant that **remembers your conversations**. Unlike typical chatbots that forget after each message, this app maintains full conversation history within a session using Cloudflare KV storage.

### How It Works
1. You type a message in the web interface
2. Message is sent to a Cloudflare Worker
3. Worker loads your previous conversation from KV
4. AI analyzes the full history + your new message
5. AI responds with context-aware answer
6. Entire conversation (user + AI) is saved back to KV
7. Next time you chat, the AI remembers everything

---

## âœ¨ Key Features

- **Conversation Memory:** Full history maintained per session (24-hour TTL)
- **Llama 3.1 8B Model:** Superior instruction-following and natural conversation
- **Edge Computing:** 300+ global data centers = fast responses from anywhere
- **Zero Cold Starts:** Instant responses, no Lambda warm-up delays
- **Pay Per Use:** Only charged for actual compute time, not idle servers
- **No API Keys Needed:** Works directly with Cloudflare bindings
- **Modern UI:** Dark mode, loading animations, Cloudflare-inspired design
- **Production Ready:** CORS configured, error handling, graceful degradation

---

## ğŸ¤– Why Llama 3.1 8B?

I upgraded from Mistral 7B to **Llama 3.1 8B** for several key reasons:

### **Assignment Alignment**
- âœ… Cloudflare assignment recommends using **Llama 3.3** (or similar)
- âœ… Llama 3.1 8B is the closest available model on Workers AI
- âœ… Shows we follow best practices and recommendations

### **Better Performance**
- **Instruction Following:** Llama 3.1 respects system prompts much better than Mistral
- **Natural Conversations:** More human-like responses, less robotic
- **Reliability:** Consistently completes responses without truncation issues
- **Smart Context Handling:** Intelligently adapts to large list requests

### **Real-World Impact**
```
Before (Mistral 7B):
User: "Give me 10 reasons..."
AI: Lists 7 items, then gets cut off âŒ

After (Llama 3.1 8B):
User: "Give me 10 reasons..."
AI: Provides 3-5 key points + "Want more?" âœ…
```

**Result:** Users get complete, useful responses every time instead of incomplete lists.

---

## ğŸŒ Deployment URLs

**Live Application:**
- **Frontend:** https://c7969915.cf-ai-chat-memory.pages.dev (connect via GitHub to Cloudflare Pages)
- **Worker API:** https://cf-ai-chat-memory.ankushrai37.workers.dev âœ… (already deployed)
- **GitHub:** https://github.com/akrai37/cf_ai_chat_memory

**Important Note:** Locally you develop both components together in one project, but Cloudflare deploys them as **two separate applications** for better scalability and performance.

---

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+
- Cloudflare account (free tier works!)
- Wrangler CLI

### 1ï¸âƒ£ Install Wrangler
```bash
npm install -g wrangler
# or use npx wrangler for each command
```

### 2ï¸âƒ£ Set Up KV Namespace
```bash
cd workers
npx wrangler kv namespace create SESSIONS
# Copy the returned 'id' into wrangler.toml under [[kv_namespaces]]
```

Your `wrangler.toml` should look like:
```toml
[[kv_namespaces]]
binding = "SESSIONS"
id = "your-id-here"
preview_id = "your-preview-id-here"
```

### 3ï¸âƒ£ Start the Backend (Worker)
```bash
cd workers
npx wrangler dev src/index.js
# Server runs at http://127.0.0.1:8787
```

### 4ï¸âƒ£ Start the Frontend
```bash
# In a new terminal
cd web
npx serve . -p 3001
# Opens at http://localhost:3001
```

### 5ï¸âƒ£ Test It Out
- Go to `http://localhost:3001`
- Type a message like "Hi! I'm building a chat app"
- Ask follow-up: "What was I building?" â†’ AI remembers!

---

## ğŸ“¦ Project Structure

```
cf_ai_chat_memory/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ PROMPTS.md                   # AI prompts + development history
â”œâ”€â”€ personal_notes.txt           # Quick reference guide
â”œâ”€â”€ workers/
â”‚   â”œâ”€â”€ wrangler.toml           # Worker config + KV bindings
â”‚   â”œâ”€â”€ package.json            
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ index.js            # Cloudflare Worker code (main logic)
â””â”€â”€ web/
    â””â”€â”€ index.html              # Chat UI (one file, no build needed)
```

---

## âš™ï¸ Configuration

### Change the AI Model
Edit `workers/src/index.js` line 32:
```javascript
const MODEL_ID = "@cf/meta/llama-3.1-8b-instruct";
```

Available Workers AI models:
- `@cf/meta/llama-3.1-8b-instruct` (current - recommended by assignment)
- `@cf/meta/llama-3-8b-instruct`
- `@cf/mistral/mistral-7b-instruct-v0.1`
- See more at [Workers AI Docs](https://developers.cloudflare.com/workers-ai/models/)

### Adjust Session Settings
Edit `workers/src/index.js`:
- **TTL:** Line 51 `expirationTtl: 60 * 60 * 24` (change 24 for different hours)
- **Memory:** Line 49 `history.slice(-10)` (change 10 for more/fewer messages)
- **CORS:** Line 9 `"Access-Control-Allow-Origin": "*"` (restrict in production)

---

## ğŸŒ Deploy to Production

### Deploy Worker (Already Done âœ…)
```bash
cd workers
npx wrangler deploy
# Your Worker URL: https://cf-ai-chat-memory.ankushrai37.workers.dev
```

### Deploy Frontend to Cloudflare Pages

**Easiest Method: Connect GitHub**
1. Go to [Cloudflare Dashboard](https://dash.cloudflare.com)
2. Select **Pages** â†’ **Connect to Git**
3. Authorize GitHub and select `cf_ai_chat_memory` repo
4. Set **Build settings:**
   - Build command: (leave empty)
   - Build output directory: `web`
5. Click **Save and Deploy**
6. Your site will be available at: `cf-ai-chat-memory.pages.dev`

**Manual Deployment:**
```bash
cd web
npx wrangler pages deploy . --project-name cf-ai-chat-memory
```

### Update WORKER_URL After Deployment
Once your frontend is deployed, update the Worker URL in `web/index.html`:
```javascript
const WORKER_URL = "https://cf-ai-chat-memory.ankushrai37.workers.dev";
```

Then commit and push to GitHub - Pages will auto-redeploy!

---

## ï¿½ Understanding Session Memory

### How Memory Works
- **Session ID:** Generated randomly when you load the page
- **Storage:** Stored in Cloudflare KV with 24-hour expiration
- **Scope:** Memory is **per-session**, not per-user
- **History Limit:** Keeps last 10 messages to stay within limits

### Important Limitations âš ï¸

**What happens when you refresh the page?**
- âŒ Refreshing generates a NEW session ID
- âŒ Old chat history is orphaned
- âŒ You start with a fresh conversation

**Example:**
```
Session 1: "My name is Alice"
[Refresh page]
Session 2: "What's my name?" â†’ AI doesn't know (different session)
```

### Current Architecture
This app uses **stateless, temporary sessions** by design:
- Great for demos, testing, and learning Cloudflare
- Perfect for short-lived conversations
- No authentication needed
- Minimal infrastructure

### Future Enhancement Options
To retain memory across page refreshes:

1. **localStorage (Simple - No Backend Needed)**
   - Save session ID in browser storage
   - Same browser = same conversation
   - Easy to implement

2. **URL-based Sessions (Shareable)**
   - Generate URL: `?sessionId=abc123`
   - Bookmark to keep conversation
   - Share with others

3. **User Authentication (Production)**
   - Sign-in system (GitHub/Google OAuth)
   - Memory tied to user account
   - Access anywhere, anytime

---

| Feature | Cloudflare | AWS Lambda |
|---------|-----------|----------|
| **Location** | 300+ edge locations | Regional data centers |
| **Cold Start** | Instant | 1-5+ seconds |
| **Cost Model** | Pay-per-execution | Provisioned capacity |
| **Setup** | Simple (wrangler) | Complex (CDK/Terraform) |
| **AI Integration** | Built-in Workers AI | Need SageMaker (expensive) |
| **KV Storage** | Global replication | DynamoDB (higher costs) |
| **Time to Deploy** | ~30 seconds | 5-15 minutes |

**Result:** Your chat app is faster, cheaper, and simpler to maintain on Cloudflare.

---

## ğŸ§ª Testing

### Test the API directly
```bash
curl -X POST http://127.0.0.1:8787/chat \
  -H "Content-Type: application/json" \
  -H "x-session-id: test-user" \
  -d '{"message":"What is your name?"}'
```

### Test memory persistence
1. Send: "My name is Alice"
2. Send: "What's my name?" â†’ Should respond "Alice"
3. Refresh page, send: "What's my name?" â†’ Still remembers! ğŸ‰

---

## ğŸ“š Additional Resources

- [Cloudflare Workers Docs](https://developers.cloudflare.com/workers/)
- [Workers AI Models](https://developers.cloudflare.com/workers-ai/models/)
- [Cloudflare KV Storage](https://developers.cloudflare.com/workers/runtime-apis/kv/)
- [Wrangler CLI Docs](https://developers.cloudflare.com/workers/wrangler/)

---

## âœ… Submission Checklist (Cloudflare Assignment)

- âœ… Repo name starts with `cf_ai_` 
- âœ… `README.md` with clear setup & deployment instructions
- âœ… `PROMPTS.md` documenting all AI prompts used
- âœ… Uses Workers AI (Llama 3.1 8B LLM - assignment recommended)
- âœ… Workflow/coordination layer (Cloudflare Worker)
- âœ… User input via chat UI (web interface)
- âœ… Memory/state (Cloudflare KV persistent storage)
- âœ… Original code (100% custom built)
- âœ… Running locally and deployable to production

---

## ğŸ“ License & Attribution

Built for the Cloudflare Workers AI Assignment. All code is original.

**Questions?** Check `personal_notes.txt` for a quick reference!
